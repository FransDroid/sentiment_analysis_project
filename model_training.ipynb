{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d1df5f",
   "metadata": {},
   "source": [
    "# Simplified Sentiment Analysis Model Training\n",
    "\n",
    "A streamlined approach to train a custom sentiment analysis model without API keys or complex setup.\n",
    "\n",
    "## Quick Overview\n",
    "- **Dataset**: Cardiff NLP Tweet Eval (60k tweets)\n",
    "- **Model**: Twitter-optimized RoBERTa\n",
    "- **Training**: 3 epochs, no external services required\n",
    "- **Output**: Ready-to-use model for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5e756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies installed - no API keys required!\n"
     ]
    }
   ],
   "source": [
    "# Quick Setup - Install dependencies and disable wandb\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable wandb to avoid API key prompts\n",
    "\n",
    "!pip install -q transformers datasets torch scikit-learn matplotlib seaborn\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31d3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chief/projects/python/sentiment_analysis_project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import everything we need\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"üöÄ Setup complete! Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"üìÖ Training started: {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data (this takes ~30 seconds)\n",
    "print(\"üì• Loading dataset...\")\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "# Quick preprocessing\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\w+', '[URL]', text)  # URLs and mentions\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)  # Remove # from hashtags\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_batch(examples):\n",
    "    examples['text'] = [clean_text(text) for text in examples['text']]\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(preprocess_batch, batched=True)\n",
    "print(f\"‚úÖ Data loaded: {len(dataset['train'])} train, {len(dataset['validation'])} val, {len(dataset['test'])} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model and tokenizer\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "print(f\"ü§ñ Loading model: {MODEL_NAME}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"NEUTRAL\": 1, \"POSITIVE\": 2}\n",
    ")\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_batch(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_batch, batched=True, remove_columns=['text'])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"‚úÖ Model and data ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1606c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./simple_sentiment_model',\n",
    "    num_train_epochs=2,                    # Reduced for faster training\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,\n",
    "    eval_strategy=\"epoch\",                 # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,              # Simplified for stability\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[],                          # No external reporting\n",
    ")\n",
    "\n",
    "# Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "print(\"‚öôÔ∏è Training configuration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad77f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (takes ~5-10 minutes)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "training_result = trainer.train()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed in {duration:.1f} minutes!\")\n",
    "print(f\"üìä Final training loss: {training_result.training_loss:.4f}\")\n",
    "print(f\"üéØ Training steps: {training_result.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53574c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation and save\n",
    "print(\"üìà Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_dataset['test'])\n",
    "\n",
    "print(f\"üéØ Test Results:\")\n",
    "print(f\"   Accuracy: {test_results['eval_accuracy']:.3f}\")\n",
    "print(f\"   F1 Score: {test_results['eval_f1']:.3f}\")\n",
    "\n",
    "# Save the trained model\n",
    "save_path = \"./trained_sentiment_model\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"\\nüíæ Model saved to: {save_path}\")\n",
    "print(\"üéâ Ready to use in your sentiment analysis project!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with sample texts\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    \"I love this product! It's amazing!\",\n",
    "    \"This is terrible, worst experience ever.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"@company thanks for the great service! #happy\",\n",
    "    \"Check this out: https://example.com not sure about it\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing your new model:\")\n",
    "print(\"=\" * 50)\n",
    "for text in test_texts:\n",
    "    clean = clean_text(text)\n",
    "    result = sentiment_analyzer(clean)[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {result['label']} ({result['score']:.3f})\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc221c1",
   "metadata": {},
   "source": [
    "# Integration Instructions\n",
    "\n",
    "Now you can use your trained model in your project! Here's how:\n",
    "\n",
    "## Simple Usage (copy this code):\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"./trained_sentiment_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Create analyzer\n",
    "analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Use it\n",
    "result = analyzer(\"I love this product!\")\n",
    "print(result)  # [{'label': 'POSITIVE', 'score': 0.999}]\n",
    "```\n",
    "\n",
    "## Integration Steps:\n",
    "1. Copy the `trained_sentiment_model` folder to your project\n",
    "2. Replace TextBlob in `src/sentiment_analysis/sentiment_analyzer.py` \n",
    "3. Update `config/settings.py` to use the new model\n",
    "4. Test with your existing data pipeline\n",
    "\n",
    "**That's it! No API keys, no complex setup required.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689595c",
   "metadata": {},
   "source": [
    "# Custom Sentiment Analysis Model Training\n",
    "\n",
    "This notebook trains a custom sentiment analysis model using the cardiffnlp/tweet_eval dataset from Hugging Face. The trained model will replace the default TextBlob analyzer in our sentiment analysis project for improved accuracy on social media text.\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: Cardiff NLP Tweet Eval Dataset\n",
    "- **Task**: Sentiment Classification\n",
    "- **Labels**: 3 classes (Negative, Neutral, Positive)\n",
    "- **Size**: ~60k tweets\n",
    "- **Language**: English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbc15b",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, we'll install the necessary libraries and import the required modules for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers datasets vaderSentiment\n",
    "!pip install scikit-learn matplotlib seaborn accelerate\n",
    "# Optional: experiment tracking\n",
    "!pip install wandb\n",
    "\n",
    "# Verify PyTorch installation and GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Scikit-learn for metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11882b",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Tweet Eval Dataset\n",
    "\n",
    "Let's load the tweet_eval sentiment dataset and explore its structure, distribution, and sample content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweet_eval sentiment dataset\n",
    "print(\"Loading tweet_eval sentiment dataset...\")\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"\\nDataset features: {dataset['train'].features}\")\n",
    "print(f\"\\nTraining samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore label distribution\n",
    "def explore_dataset_labels(dataset_split, split_name):\n",
    "    \"\"\"Explore and visualize label distribution in dataset split\"\"\"\n",
    "    labels = dataset_split['label']\n",
    "    label_names = ['Negative', 'Neutral', 'Positive']\n",
    "    \n",
    "    # Count labels\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label_names[label]] = label_counts.get(label_names[label], 0) + 1\n",
    "    \n",
    "    print(f\"\\n{split_name} Set Label Distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / len(labels)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return label_counts\n",
    "\n",
    "# Explore all splits\n",
    "train_counts = explore_dataset_labels(dataset['train'], 'Training')\n",
    "val_counts = explore_dataset_labels(dataset['validation'], 'Validation')\n",
    "test_counts = explore_dataset_labels(dataset['test'], 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e205df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "splits = [('Training', train_counts), ('Validation', val_counts), ('Test', test_counts)]\n",
    "\n",
    "for idx, (split_name, counts) in enumerate(splits):\n",
    "    labels = list(counts.keys())\n",
    "    values = list(counts.values())\n",
    "    colors = ['#ff6b6b', '#ffd93d', '#6bcf7f']  # Red, Yellow, Green\n",
    "    \n",
    "    axes[idx].pie(values, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    axes[idx].set_title(f'{split_name} Set Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample tweets from each category\n",
    "label_names = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "print(\"Sample tweets by sentiment category:\\n\")\n",
    "for label_idx, label_name in enumerate(label_names):\n",
    "    print(f\"=== {label_name.upper()} TWEETS ===\")\n",
    "    \n",
    "    # Find tweets with this label\n",
    "    samples = []\n",
    "    for i, item in enumerate(dataset['train']):\n",
    "        if item['label'] == label_idx and len(samples) < 3:\n",
    "            samples.append(item['text'])\n",
    "    \n",
    "    for i, text in enumerate(samples, 1):\n",
    "        print(f\"{i}. {text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216a13e",
   "metadata": {},
   "source": [
    "## 3. Preprocess Text Data\n",
    "\n",
    "We'll clean and preprocess the tweet text to handle URLs, mentions, hashtags, and other social media-specific elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def preprocess_tweet(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess tweet text by cleaning URLs, mentions, hashtags, etc.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw tweet text\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned tweet text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace URLs with [URL] token\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n",
    "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n",
    "    \n",
    "    # Replace user mentions with [USER] token\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '[USER]', text)\n",
    "    \n",
    "    # Keep hashtags but remove the # symbol\n",
    "    text = re.sub(r'#([A-Za-z0-9_]+)', r'\\1', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test preprocessing function\n",
    "sample_tweets = [\n",
    "    \"I love this product! #amazing @company https://example.com\",\n",
    "    \"@user This is terrible... why would anyone buy this? #disappointed\",\n",
    "    \"It's okay I guess. Nothing special. Check out www.example.com for more info\"\n",
    "]\n",
    "\n",
    "print(\"Preprocessing examples:\")\n",
    "for original in sample_tweets:\n",
    "    cleaned = preprocess_tweet(original)\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Cleaned:  {cleaned}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9fa646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the entire dataset\n",
    "def preprocess_dataset(examples):\n",
    "    \"\"\"Preprocess a batch of examples\"\"\"\n",
    "    examples['text'] = [preprocess_tweet(text) for text in examples['text']]\n",
    "    return examples\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "print(\"Preprocessing dataset splits...\")\n",
    "processed_dataset = dataset.map(preprocess_dataset, batched=True)\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(f\"Sample processed tweet: {processed_dataset['train'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8da4e",
   "metadata": {},
   "source": [
    "## 4. Split Dataset and Create Data Loaders\n",
    "\n",
    "We'll use the existing train/validation/test splits and create tokenized datasets with appropriate data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    print(\"‚úì AutoTokenizer imported successfully\")\n",
    "    \n",
    "    # Initialize tokenizer right away\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"  # Pre-trained on tweets\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"‚úì Tokenizer loaded: {MODEL_NAME}\")\n",
    "    print(f\"‚úì Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing transformers: {e}\")\n",
    "    print(\"Installing transformers...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"transformers\"])\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Import other components when needed (lazy loading approach)\n",
    "# This spreads the import time across multiple cells instead of one big import\n",
    "\n",
    "def lazy_import_transformers():\n",
    "    \"\"\"Import transformers components as needed\"\"\"\n",
    "    global AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "    global DataCollatorWithPadding, pipeline\n",
    "    \n",
    "    print(\"üîÑ Loading remaining transformers components...\")\n",
    "    \n",
    "    from transformers import (\n",
    "        AutoModelForSequenceClassification,\n",
    "        Trainer, \n",
    "        TrainingArguments,\n",
    "        DataCollatorWithPadding,\n",
    "        pipeline\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ All transformers components loaded!\")\n",
    "    return True\n",
    "\n",
    "# Call this function when you actually need these components\n",
    "# lazy_import_transformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    print(\"‚úì VADER available - ultra-fast sentiment analysis option\")\n",
    "    \n",
    "    # Quick test\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sample = \"I love this product!\"\n",
    "    score = analyzer.polarity_scores(sample)\n",
    "    print(f\"VADER test: '{sample}' -> {score}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"üí° Install VADER for ultra-fast alternative: pip install vaderSentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text data\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=128,  # Tweets are typically short\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# Apply tokenization to all splits\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove original text to save memory\n",
    ")\n",
    "\n",
    "print(\"Tokenization completed!\")\n",
    "print(f\"Tokenized dataset structure: {tokenized_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST ALTERNATIVE: Create a simple data collator without importing DataCollatorWithPadding\n",
    "class SimpleDataCollator:\n",
    "    \"\"\"Lightweight alternative to DataCollatorWithPadding\"\"\"\n",
    "    def __init__(self, tokenizer, padding=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = padding\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        import torch\n",
    "        # Get all the keys from the first feature\n",
    "        first = features[0]\n",
    "        batch = {}\n",
    "        \n",
    "        # Handle each key in the features\n",
    "        for key in first.keys():\n",
    "            values = [f[key] for f in features]\n",
    "            \n",
    "            if key == \"input_ids\" or key == \"attention_mask\":\n",
    "                # Convert to tensors and pad if needed\n",
    "                if self.padding:\n",
    "                    # Convert to lists first if they're tensors\n",
    "                    if isinstance(values[0], torch.Tensor):\n",
    "                        values = [v.tolist() if isinstance(v, torch.Tensor) else v for v in values]\n",
    "                    \n",
    "                    # Find max length in batch\n",
    "                    max_length = max(len(v) for v in values)\n",
    "                    # Pad sequences\n",
    "                    padded_values = []\n",
    "                    for v in values:\n",
    "                        if len(v) < max_length:\n",
    "                            # Pad with tokenizer's pad_token_id for input_ids, 0 for attention_mask\n",
    "                            pad_value = self.tokenizer.pad_token_id if key == \"input_ids\" else 0\n",
    "                            padded = v + [pad_value] * (max_length - len(v))\n",
    "                            padded_values.append(padded)\n",
    "                        else:\n",
    "                            padded_values.append(v)\n",
    "                    batch[key] = torch.tensor(padded_values)\n",
    "                else:\n",
    "                    # Stack tensors or convert lists to tensors\n",
    "                    if isinstance(values[0], torch.Tensor):\n",
    "                        batch[key] = torch.stack(values)\n",
    "                    else:\n",
    "                        batch[key] = torch.tensor(values)\n",
    "            else:\n",
    "                # For labels and other non-sequence data\n",
    "                if isinstance(values[0], torch.Tensor):\n",
    "                    batch[key] = torch.stack(values)\n",
    "                else:\n",
    "                    batch[key] = torch.tensor(values)\n",
    "                \n",
    "        return batch\n",
    "\n",
    "# Set dataset format for PyTorch\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Create our fast data collator\n",
    "data_collator = SimpleDataCollator(tokenizer)\n",
    "\n",
    "print(\"‚úÖ Fast data collator created!\")\n",
    "\n",
    "# Display dataset statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Training set: {len(tokenized_dataset['train'])} samples\")\n",
    "print(f\"Validation set: {len(tokenized_dataset['validation'])} samples\")\n",
    "print(f\"Test set: {len(tokenized_dataset['test'])} samples\")\n",
    "\n",
    "# Show sample tokenized data\n",
    "sample = tokenized_dataset['train'][0]\n",
    "print(f\"\\nSample tokenized data:\")\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready to proceed without waiting for DataCollatorWithPadding import!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996c096",
   "metadata": {},
   "source": [
    "## 5. Define the Model Architecture\n",
    "\n",
    "We'll use a pre-trained RoBERTa model fine-tuned for Twitter sentiment analysis as our base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,  # 3 sentiment classes\n",
    "    id2label={0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
    "    label2id={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    ")\n",
    "\n",
    "# Display model information\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Number of labels: {model.num_labels}\")\n",
    "print(f\"Label mapping: {model.config.id2label}\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model moved to device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebe0b6",
   "metadata": {},
   "source": [
    "## 6. Set Up Training Configuration\n",
    "\n",
    "Configure training parameters including learning rate, epochs, batch size, and other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                    # Output directory\n",
    "    num_train_epochs=3,                        # Number of training epochs\n",
    "    per_device_train_batch_size=16,           # Batch size for training\n",
    "    per_device_eval_batch_size=32,            # Batch size for evaluation\n",
    "    warmup_steps=500,                         # Number of warmup steps\n",
    "    weight_decay=0.01,                        # Strength of weight decay\n",
    "    learning_rate=2e-5,                       # Learning rate\n",
    "    logging_dir='./logs',                     # Directory for storing logs\n",
    "    logging_steps=100,                        # Log every N steps\n",
    "    eval_strategy=\"steps\",                    # Changed from evaluation_strategy\n",
    "    eval_steps=500,                           # Evaluation frequency\n",
    "    save_strategy=\"steps\",                    # Save every N steps\n",
    "    save_steps=1000,                          # Save frequency\n",
    "    load_best_model_at_end=True,             # Load best model at end\n",
    "    metric_for_best_model=\"eval_accuracy\",    # Metric for best model\n",
    "    greater_is_better=True,                   # Higher accuracy is better\n",
    "    save_total_limit=3,                       # Only save last N checkpoints\n",
    "    seed=42,                                  # Random seed\n",
    "    fp16=torch.cuda.is_available(),           # Use mixed precision if GPU available\n",
    "    dataloader_num_workers=4,                 # Number of data loading workers\n",
    "    remove_unused_columns=False,              # Keep all columns\n",
    "    push_to_hub=False,                        # Don't push to HuggingFace Hub\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Eval batch size: {training_args.per_device_eval_batch_size}\")\n",
    "print(f\"  Mixed precision: {training_args.fp16}\")\n",
    "print(\"‚úÖ Training arguments configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy, precision, recall, and F1 score\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "print(\"Metrics computation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668904e5",
   "metadata": {},
   "source": [
    "## 7. Train the Sentiment Analysis Model\n",
    "\n",
    "Implement the training loop with progress tracking and validation monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33913534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Training dataset size: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(tokenized_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# Start training\n",
    "print(\"Starting model training...\")\n",
    "print(f\"Training start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Train the model\n",
    "training_result = trainer.train()\n",
    "\n",
    "print(f\"Training completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Training loss: {training_result.training_loss:.4f}\")\n",
    "print(f\"Total training steps: {training_result.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbbeeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "# Extract training and validation metrics\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "eval_accuracy = []\n",
    "eval_f1 = []\n",
    "steps = []\n",
    "\n",
    "for log in training_logs:\n",
    "    if 'loss' in log:  # Training logs\n",
    "        train_loss.append(log['loss'])\n",
    "        steps.append(log['step'])\n",
    "    elif 'eval_loss' in log:  # Evaluation logs\n",
    "        eval_loss.append(log['eval_loss'])\n",
    "        eval_accuracy.append(log['eval_accuracy'])\n",
    "        eval_f1.append(log['eval_f1'])\n",
    "\n",
    "# Create training plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training loss\n",
    "axes[0, 0].plot(steps, train_loss, 'b-', label='Training Loss')\n",
    "axes[0, 0].set_title('Training Loss Over Time')\n",
    "axes[0, 0].set_xlabel('Steps')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Validation loss\n",
    "if eval_loss:\n",
    "    eval_steps = [log['step'] for log in training_logs if 'eval_loss' in log]\n",
    "    axes[0, 1].plot(eval_steps, eval_loss, 'r-', label='Validation Loss')\n",
    "    axes[0, 1].set_title('Validation Loss Over Time')\n",
    "    axes[0, 1].set_xlabel('Steps')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "# Validation accuracy\n",
    "if eval_accuracy:\n",
    "    axes[1, 0].plot(eval_steps, eval_accuracy, 'g-', label='Validation Accuracy')\n",
    "    axes[1, 0].set_title('Validation Accuracy Over Time')\n",
    "    axes[1, 0].set_xlabel('Steps')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "# Validation F1 score\n",
    "if eval_f1:\n",
    "    axes[1, 1].plot(eval_steps, eval_f1, 'purple', label='Validation F1')\n",
    "    axes[1, 1].set_title('Validation F1 Score Over Time')\n",
    "    axes[1, 1].set_xlabel('Steps')\n",
    "    axes[1, 1].set_ylabel('F1 Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00fd67",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance\n",
    "\n",
    "Evaluate the trained model on the test dataset and calculate comprehensive performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60545a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_dataset['test'])\n",
    "\n",
    "print(\"Test Set Results:\")\n",
    "for key, value in test_results.items():\n",
    "    if key.startswith('eval_'):\n",
    "        metric_name = key.replace('eval_', '').title()\n",
    "        print(f\"  {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e46ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed predictions for confusion matrix\n",
    "predictions = trainer.predict(tokenized_dataset['test'])\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Calculate detailed metrics\n",
    "report = classification_report(\n",
    "    y_true, y_pred, \n",
    "    target_names=['Negative', 'Neutral', 'Positive'],\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Display detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"-\" * 50)\n",
    "for label in ['Negative', 'Neutral', 'Positive']:\n",
    "    metrics = report[label]\n",
    "    print(f\"{label}:\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-score:  {metrics['f1-score']:.4f}\")\n",
    "    print(f\"  Support:   {int(metrics['support'])}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Overall Accuracy: {report['accuracy']:.4f}\")\n",
    "print(f\"Macro Average F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted Average F1: {report['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Add percentage annotations\n",
    "total = cm.sum()\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        percentage = (cm[i, j] / total) * 100\n",
    "        plt.text(j + 0.7, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=10, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"  {label}: {class_accuracy[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80074daa",
   "metadata": {},
   "source": [
    "## 9. Save and Export the Trained Model\n",
    "\n",
    "Save the trained model, tokenizer, and configuration files for use in the sentiment analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving the model\n",
    "model_save_path = \"./trained_sentiment_model\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "print(\"Saving trained model and tokenizer...\")\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(f\"Files saved:\")\n",
    "for file in os.listdir(model_save_path):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration file for the project\n",
    "model_config = {\n",
    "    \"model_name\": \"custom_twitter_sentiment\",\n",
    "    \"model_path\": model_save_path,\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"num_labels\": 3,\n",
    "    \"label_mapping\": {0: \"negative\", 1: \"neutral\", 2: \"positive\"},\n",
    "    \"preprocessing_required\": True,\n",
    "    \"max_length\": 128,\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"test_accuracy\": float(test_results['eval_accuracy']),\n",
    "    \"test_f1\": float(test_results['eval_f1']),\n",
    "    \"model_size_mb\": sum(os.path.getsize(os.path.join(model_save_path, f)) \n",
    "                        for f in os.listdir(model_save_path)) / (1024 * 1024)\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "import json\n",
    "config_path = os.path.join(model_save_path, \"model_config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(f\"Model configuration saved to: {config_path}\")\n",
    "print(\"Configuration:\")\n",
    "for key, value in model_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c24c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test samples representing different sentiment types\n",
    "test_samples = [\n",
    "    # Positive samples\n",
    "    \"I absolutely love this new feature! It's amazing! üéâ\",\n",
    "    \"Best product ever! Highly recommended to everyone!\",\n",
    "    \"Great job team! This update is fantastic!\",\n",
    "    \n",
    "    # Negative samples  \n",
    "    \"This is terrible. Worst experience ever. üòû\",\n",
    "    \"I hate this app. It never works properly!\",\n",
    "    \"Completely disappointed. Waste of money.\",\n",
    "    \n",
    "    # Neutral samples\n",
    "    \"It's okay, nothing special but works fine.\",\n",
    "    \"The product arrived on time. Standard quality.\",\n",
    "    \"Meeting scheduled for tomorrow at 3 PM.\",\n",
    "    \n",
    "    # Social media specific\n",
    "    \"@company your service is down again #frustrated\",\n",
    "    \"Check out this link https://example.com not sure about it\",\n",
    "    \"Thanks @support for the quick response! #grateful\"\n",
    "]\n",
    "\n",
    "print(\"Testing Custom Model Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(test_samples, 1):\n",
    "    # Get prediction from our custom model\n",
    "    result = sentiment_pipeline(preprocess_tweet(text))[0]\n",
    "    \n",
    "    print(f\"\\n{i}. Text: {text}\")\n",
    "    print(f\"   Prediction: {result['label']} (confidence: {result['score']:.4f})\")\n",
    "    \n",
    "    # Show preprocessed text if different\n",
    "    preprocessed = preprocess_tweet(text)\n",
    "    if preprocessed != text.lower():\n",
    "        print(f\"   Preprocessed: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baseline TextBlob (if available)\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    \n",
    "    def textblob_sentiment(text):\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        \n",
    "        if polarity > 0.1:\n",
    "            return \"positive\"\n",
    "        elif polarity < -0.1:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPARISON: Custom Model vs TextBlob Baseline\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    comparison_samples = test_samples[:6]  # Use first 6 samples\n",
    "    \n",
    "    for i, text in enumerate(comparison_samples, 1):\n",
    "        # Custom model prediction\n",
    "        custom_result = sentiment_pipeline(preprocess_tweet(text))[0]\n",
    "        custom_sentiment = custom_result['label'].lower()\n",
    "        custom_confidence = custom_result['score']\n",
    "        \n",
    "        # TextBlob prediction\n",
    "        textblob_sentiment_result = textblob_sentiment(text)\n",
    "        \n",
    "        # Agreement indicator\n",
    "        agreement = \"‚úì\" if custom_sentiment == textblob_sentiment_result else \"‚úó\"\n",
    "        \n",
    "        print(f\"\\n{i}. {text}\")\n",
    "        print(f\"   Custom Model:  {custom_sentiment} ({custom_confidence:.4f})\")\n",
    "        print(f\"   TextBlob:      {textblob_sentiment_result}\")\n",
    "        print(f\"   Agreement:     {agreement}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nTextBlob not installed. Install with: pip install textblob\")\n",
    "    print(\"Skipping baseline comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdce33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary and recommendations\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Model Performance:\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Test F1 Score: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Model Size: {model_config['model_size_mb']:.1f} MB\")\n",
    "\n",
    "print(f\"\\nüîß Integration Steps:\")\n",
    "print(f\"   1. Copy model files from: {model_save_path}\")\n",
    "print(f\"   2. Update src/sentiment_analysis/sentiment_analyzer.py\")\n",
    "print(f\"   3. Use the integration code provided above\")\n",
    "print(f\"   4. Update config/settings.py to use custom model\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance Optimizations:\")\n",
    "print(f\"   ‚Ä¢ Model is optimized for Twitter/social media text\")\n",
    "print(f\"   ‚Ä¢ Preprocessing pipeline handles URLs, mentions, hashtags\")\n",
    "print(f\"   ‚Ä¢ GPU acceleration available if CUDA is present\")\n",
    "print(f\"   ‚Ä¢ Confidence scores provided for filtering low-quality predictions\")\n",
    "\n",
    "print(f\"\\nüéØ Recommendations:\")\n",
    "print(f\"   ‚Ä¢ Use confidence threshold of 0.7+ for high-quality predictions\")\n",
    "print(f\"   ‚Ä¢ Monitor model performance on new data\")\n",
    "print(f\"   ‚Ä¢ Consider retraining if accuracy drops below 0.80\")\n",
    "print(f\"   ‚Ä¢ Implement A/B testing between custom model and TextBlob\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed successfully!\")\n",
    "print(f\"   Model ready for production use in sentiment analysis project.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
